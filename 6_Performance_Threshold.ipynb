{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance: AUROC Barplots\n",
    "## Lawrence He and Felipe Giuste (2022-09-09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Classifiers ##\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import load\n",
    "\n",
    "## Metrics ##\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, f1_score, accuracy_score, recall_score, precision_score\n",
    "from scipy.stats import wilcoxon \n",
    "\n",
    "### Seed ###\n",
    "random_state = 1234\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Folder paths ##\n",
    "data_folder = '../Data/'\n",
    "results_folder='../Results/'\n",
    "models_folder='../Models'\n",
    "figures_folder ='../Figures/Performance/'\n",
    "\n",
    "## Outcome variable ##\n",
    "outcome_column='ckd_status'\n",
    "## Feature Ranks ##\n",
    "\n",
    "## Risk Threshold Increment ## \n",
    "threshold_increment=1e-3\n",
    "thresholds = np.arange(0, 1+threshold_increment, threshold_increment)\n",
    "n_thresholds = len(thresholds)\n",
    "\n",
    "## Test Performance ## \n",
    "performance_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep Learning Model Classifiers ###\n",
    "deep_model_names = [\n",
    "    'relu',\n",
    "    'softmax',\n",
    "    'sigmoid',\n",
    "    'gumbel_softmax',\n",
    "]\n",
    "\n",
    "## Conventional Classifier dictionary ##\n",
    "conventional_classifier_dict={'LogisticRegression':LogisticRegression, \n",
    "                 'RandomForestClassifier':RandomForestClassifier, \n",
    "                 'AdaBoostClassifier':AdaBoostClassifier, \n",
    "                 'GaussianNB':GaussianNB, \n",
    "                 'KNeighborsClassifier':KNeighborsClassifier, \n",
    "                 'SVC_radial':SVC, \n",
    "                 'XGBoostClassifier':XGBClassifier\n",
    "                }\n",
    "conventional_model_names = list(conventional_classifier_dict.keys())\n",
    "\n",
    "## Cleanup model names ##\n",
    "model_name_map = {'LogisticRegression':'Logistic Regression',\n",
    "                  'RandomForestClassifier':'Random Forest',\n",
    "                  'AdaBoostClassifier':'AdaBoost Classifier',\n",
    "                  'GaussianNB':'Gaussian NB',\n",
    "                  'KNeighborsClassifier':'KNN',\n",
    "                  'SVC_radial':'SVC',\n",
    "                  'XGBoostClassifier':'XGBoost',\n",
    "                  'relu':'NN: ReLu',\n",
    "                  'softmax':'NN: Softmax',\n",
    "                  'sigmoid':'NN: Sigmoid',\n",
    "                  'gumbel_softmax':'NN: Gumbel Softmax',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/qxp2fcwj00bff0rdg3hmhyx80000gn/T/ipykernel_66602/2873637929.py:2: DtypeWarning: Columns (30,41,42,66,67,68,69,83,87,103,104,105,121,122,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(data_folder+'train_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/qxp2fcwj00bff0rdg3hmhyx80000gn/T/ipykernel_66602/2873637929.py:3: DtypeWarning: Columns (30,41,42,66,67,68,69,83,87,103,104,105,121,122,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_data = pd.read_csv(data_folder+'test_data.csv')\n"
     ]
    }
   ],
   "source": [
    "### Load ###\n",
    "train_data = pd.read_csv(data_folder+'train_data.csv')\n",
    "test_data = pd.read_csv(data_folder+'test_data.csv')\n",
    "\n",
    "### Feature list ###\n",
    "feature_list = list()\n",
    "for i in test_data.columns:\n",
    "    if i[:2] == 'F_':\n",
    "        feature_list.append(i)\n",
    "        \n",
    "### Setup Datasets ###\n",
    "X_train = train_data[feature_list]#.rename(columns=feature_name_dict)\n",
    "y_train = train_data[outcome_column]\n",
    "X_test = test_data[feature_list]#.rename(columns=feature_name_dict)\n",
    "y_test = test_data[outcome_column]\n",
    "\n",
    "## Update Feature List ##\n",
    "feature_list = list(X_test.columns)\n",
    "print('Total Features: %s'%len(feature_list) )\n",
    "\n",
    "## Get Top-10 Features ##\n",
    "# with open(feature_ranks_path, 'rb') as fh:\n",
    "#     f_rankings_dict= pickle.load(fh)\n",
    "# features_top10 = np.array(f_rankings_dict['Feature Ranks'][:10])\n",
    "# features_top10\n",
    "\n",
    "## List of feature subsets ##\n",
    "feature_subsets = np.array(['All Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model performance on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Skip if Results Exist ##\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodels_folder\u001b[39m}\u001b[39;00m\u001b[39m/Test_Performance.pkl\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[39m## Iterate across feature subsets ##\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m feature_subset \u001b[39min\u001b[39;00m feature_subsets:\n\u001b[1;32m      6\u001b[0m         \n\u001b[1;32m      7\u001b[0m         \u001b[39m## Iterate across Classifiers ##\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m model_name_map\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_folder' is not defined"
     ]
    }
   ],
   "source": [
    "## Skip if Results Exist ##\n",
    "if(not os.path.isfile(f'{models_folder}/Test_Performance.pkl')):\n",
    "\n",
    "    ## Iterate across feature subsets ##\n",
    "    for feature_subset in feature_subsets:\n",
    "        \n",
    "        ## Iterate across Classifiers ##\n",
    "        for model_name in model_name_map.keys():\n",
    "            print(f'{feature_subset} : {model_name}          ')\n",
    "            \n",
    "            ## Dictionary keys ##\n",
    "            if(model_name not in performance_dict):\n",
    "                performance_dict[model_name] = {}\n",
    "            if(feature_subset not in performance_dict[model_name]):\n",
    "                performance_dict[model_name][feature_subset] = {}\n",
    "        \n",
    "            ## Ablated Dataset ##\n",
    "            # if(feature_subset in features_top10):\n",
    "            #     ## Ablate Feature from Top-10 ##\n",
    "            #     ablated_feature_list = features_top10[features_top10 != feature_subset]\n",
    "            #     assert( len(ablated_feature_list) == (len(features_top10)-1) )\n",
    "            #     ## Feature Ablation ##\n",
    "            #     X_train_subset = X_train[ablated_feature_list]\n",
    "            #     X_test_subset = X_test[ablated_feature_list]\n",
    "            #el\n",
    "            if(feature_subset == 'All Features'):\n",
    "                ## All ##\n",
    "                X_train_subset = X_train\n",
    "                X_test_subset= X_test\n",
    "            # elif(feature_subset == 'Top10 Features'):\n",
    "            #     ## Top-10 ##\n",
    "            #     X_train_subset = X_train[features_top10]\n",
    "            #     X_test_subset = X_test[features_top10]\n",
    "            else:\n",
    "                raise Exception('Unknown Feature Subset: %s'% feature_subset)\n",
    "            ## Check that features do not contain outcome variable ##\n",
    "            assert( outcome_column not in list(X_train_subset.columns) )\n",
    "            assert( outcome_column not in list(X_test_subset.columns) )\n",
    "            ## Update Feature List ##\n",
    "            feature_list = list(X_test_subset.columns)\n",
    "\n",
    "            ## Conventional Model ##\n",
    "            if(model_name in conventional_model_names):\n",
    "                ## Model Load ##\n",
    "                model_path = f'{models_folder}/Conventional/{model_name}.'\n",
    "                model = load(model_path) \n",
    "\n",
    "                ## Find best threshold for prediction binarization (on Training dataset) ##\n",
    "                print('\\tPredict ', end='\\r')\n",
    "                y_pred = model.predict_proba(X_train_subset)[:,1]                \n",
    "                print(f'\\tThresholding...', end='\\r')\n",
    "                mcc_list = [ matthews_corrcoef(y_true=y_train, y_pred=y_pred>thresh) for thresh in thresholds ]\n",
    "                best_thresh = thresholds[np.where( mcc_list == np.max(mcc_list) )[0]][0]\n",
    "                print(f'Best Threshold: {best_thresh}       ')\n",
    "                \n",
    "                ## Test ##\n",
    "                print('\\tTesting ', end='\\r')\n",
    "                y_pred = model.predict_proba(X_test_subset)[:,1]\n",
    "                y_pred_bin = y_pred > best_thresh\n",
    "                performance_dict[model_name][feature_subset]['y_pred'] = y_pred\n",
    "\n",
    "            ## Deep Model ##\n",
    "            # # # elif(model_name in deep_model_names):\n",
    "            # #     ## Model ID ##\n",
    "            # #     # if(feature_subset in features_top10):\n",
    "            # #     #     model_id = f'FABLATION-{feature_subset}'\n",
    "            # #     #el\n",
    "            # #     if(feature_subset == 'All Features'):\n",
    "            # #         model_id = 'ALLFEATURES'\n",
    "            # #     # elif(feature_subset == 'Top10 Features'):\n",
    "            # #     #     model_id = 'TOP10'\n",
    "            # #     else:\n",
    "            # #         raise('Unknown Feature Subset: %s'% feature_subset)\n",
    "\n",
    "            # #     ## File with Deep Learning models HP Tuning Information ##\n",
    "            # #     # hptune_df = pd.read_csv(hptune_csv)\n",
    "            # #     # ## Load Model Hyperparameters from HPTune dataframe ##\n",
    "            # #     # hyperparameters_of_interest = hptune_df[(hptune_df['penultimate_activation_type']==model_name) & \n",
    "            # #     #                                         (hptune_df['optimized_model']==True) & \n",
    "            # #     #                                         (hptune_df['outcome_variable']==outcome_column)]\n",
    "            # #     # ## Setup Hyperparameter Values ##\n",
    "            # #     # dropout1_p = hyperparameters_of_interest['dropout1_p'].values[0]\n",
    "            # #     # dropout2_p = hyperparameters_of_interest['dropout2_p'].values[0]\n",
    "            # #     # dropout3_p = hyperparameters_of_interest['dropout3_p'].values[0]\n",
    "            # #     # clustering_neurons = hyperparameters_of_interest['clustering_neurons'].values[0]\n",
    "            # #     # learning_rate = hyperparameters_of_interest['learning_rate'].values[0]\n",
    "            # #     # outcome_variable = hyperparameters_of_interest['outcome_variable'].values[0]\n",
    "            # #     # ## Model File ##\n",
    "            # #     # model_file=f'{model_id}_ACTIVATION={model_name}_CN={clustering_neurons}_D1P={dropout1_p}_D2P={dropout2_p}_D3P={dropout3_p}_LR={learning_rate}_OUTCOME={outcome_variable}'\n",
    "            # #     # ## Feature Layer Size ##\n",
    "            # #     # input_neurons=len(feature_list)\n",
    "            # #     # ## Model Load ##\n",
    "            # #     # model = DeepLearningArchitecture(dropout1_p=dropout1_p, dropout2_p=dropout2_p, dropout3_p=dropout3_p,\n",
    "            # #     #                                            clustering_neurons=clustering_neurons,\n",
    "            # #     #                                            penultimate_activation_type=model_name, input_neurons=input_neurons)\n",
    "            # #     # model.load_state_dict(torch.load(f'{models_folder}/Deep/Testing-100Trained/{model_file}_checkpoint.pth', \n",
    "            # #     #                                  map_location=device )\n",
    "            # #     #                      )\n",
    "            # #     # model.to(device)\n",
    "            # #     # model.eval()\n",
    "                \n",
    "            # #     ## Predict ##\n",
    "            # #     print('\\tPredict ', end='\\r')\n",
    "            # #     y_pred = model( torch.from_numpy(X_train_subset.values).float().to(device) ).detach().cpu().numpy()\n",
    "            # #     ## Find best threshold for prediction binarization (on Training dataset) ##\n",
    "            # #     print(f'\\tThresholding...', end='\\r')\n",
    "            # #     mcc_list = [ matthews_corrcoef(y_true=y_train, y_pred=y_pred>thresh) for thresh in thresholds ]\n",
    "            # #     best_thresh = thresholds[np.where( mcc_list == np.max(mcc_list) )[0]][0]\n",
    "            # #     print(f'Best Threshold: {best_thresh}       ')\n",
    "                \n",
    "\n",
    "            # #     ## Test ##\n",
    "            # #     print('\\tTesting ', end='\\r')\n",
    "            # #     y_pred = model( torch.from_numpy(X_test_subset.values).float().to(device) ).detach().cpu().numpy()\n",
    "            # #     y_pred_bin = y_pred > best_thresh\n",
    "            # #     performance_dict[model_name][feature_subset]['y_pred'] = y_pred\n",
    "                \n",
    "            ## Performance ##\n",
    "            performance_dict[model_name][feature_subset]['threshold'] = best_thresh\n",
    "            performance_dict[model_name][feature_subset]['AUROC'] = roc_auc_score(y_true=y_test, y_score=y_pred)\n",
    "            performance_dict[model_name][feature_subset]['MCC'] = matthews_corrcoef(y_true=y_test, y_pred=y_pred_bin)\n",
    "            performance_dict[model_name][feature_subset]['F1'] = f1_score(y_true=y_test, y_pred=y_pred_bin)\n",
    "            performance_dict[model_name][feature_subset]['accuracy'] = accuracy_score(y_true=y_test, y_pred=y_pred_bin)\n",
    "            performance_dict[model_name][feature_subset]['recall'] = recall_score(y_true=y_test, y_pred=y_pred_bin)\n",
    "            performance_dict[model_name][feature_subset]['precision'] = precision_score(y_true=y_test, y_pred=y_pred_bin)\n",
    "            ## Delete model ##\n",
    "            del(model)\n",
    "            \n",
    "    ## Save Performance Dictionary ##\n",
    "    with open( f'{models_folder}/Test_Performance.pkl', 'wb' ) as fh:\n",
    "         pickle.dump( performance_dict, file=fh )\n",
    "else:\n",
    "    print(f'Exists: {models_folder}/Test_Performance.pkl')\n",
    "    \n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
